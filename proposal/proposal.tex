\documentclass[11pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{
    CS 388 Final Project Proposal:
    Seq2Seq Architecture Extension\\}

\author{Keivaun Waugh\\
ksw734\\
{\tt\small keivaun@protonmail.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Sequence to sequence models have shown much promise for performing translations, both literal translations between languages, and translations between structures. Coupled with attention mechanisms that help the decoder learn which input words to ``focus'' on, they can achieve state-of-the-art performance. For my final project, I propose an extension to the Seq2Seq architecture that may help the network learn more easily.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

\section{Model}
FILLER

\section{Extensions}
FILLER

\section{Dataset}
FILLER

%------------------------------------------------------------------------------


{\small
\bibliographystyle{ieee}
\bibliography{proposal}
}

\end{document}
